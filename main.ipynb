{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import torch\n",
    "from torch import nn\n",
    "import spektral\n",
    "import hashlib\n",
    "from spektral.datasets.citation import Citation\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.functional import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import cos\n",
    "\n",
    "\n",
    "class CoraDataset(Dataset):\n",
    "    def __init__(self, path='data/citeseer/', context_size=10, with_wl=True, wl_iterations=2, cutoff=99) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            path (str, optional): path containing the Cora dataset. Defaults to 'data/citeseer/'.\n",
    "            context_size (int, optional): number of nodes in a target node context, represent the\n",
    "                                          topk nodes sorted by intimacy score. Defaults to 10.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.context_size = context_size\n",
    "        self.with_wl = with_wl\n",
    "        self.wl_iterations = wl_iterations\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # swap nodes on each row as they are listed as target-source\n",
    "        df = pd.read_csv(f\"{path}/citeseer.cites\", sep='\\t', header=None)\n",
    "        self.edge_list = [(x[1], x[0]) for x in df.values.tolist()]\n",
    "        self.node_list = set([x[0] for x in self.edge_list] + [x[1] for x in self.edge_list])\n",
    "        self.name_to_id = {k:v for v,k in enumerate(self.node_list)}\n",
    "        self.id_to_name = {v:k for v,k in enumerate(self.node_list)}\n",
    "        self.n = len(self.node_list)\n",
    "\n",
    "        # distances between nodes\n",
    "        ids_edge_list = [(self.name_to_id[x], self.name_to_id[y]) for x,y in self.edge_list]\n",
    "        G = nx.from_edgelist(ids_edge_list, create_using=nx.Graph)\n",
    "        self.distance_matrix = dict(nx.all_pairs_shortest_path_length(G), cutoff=cutoff)\n",
    "\n",
    "        # load features and labels\n",
    "        self.raw_features = pd.read_csv(f\"{path}/citeseer.content\", sep='\\t', header=None, index_col=0)\n",
    "        self.raw_features.index = self.raw_features.index.astype(str)\n",
    "        self.labels = self.raw_features.iloc[:,-1].astype('category').cat.codes\n",
    "        self.raw_features = self.raw_features.iloc[:,:-1]\n",
    "        self.raw_features_size = self.raw_features.shape[1]\n",
    "\n",
    "        # pre-process graph to make data loader more efficient\n",
    "        self.build_intimacy_matrix()\n",
    "        self.build_contexts()\n",
    "\n",
    "        self.wl_colors = None\n",
    "        if with_wl:\n",
    "            self.build_wl_coloring()\n",
    "        self.max_wl_colors = max(self.wl_colors.values())\n",
    "\n",
    "    def build_intimacy_matrix(self, alpha=0.15):\n",
    "        # create adjacency matrix\n",
    "        n = self.n\n",
    "        adj_mat = np.zeros((n,n))\n",
    "        name_to_id = self.name_to_id\n",
    "\n",
    "        for x,y in self.edge_list:\n",
    "            adj_mat[name_to_id[x], name_to_id[y]] = 1\n",
    "            adj_mat[name_to_id[y], name_to_id[x]] = 1\n",
    "        \n",
    "        # compute inverse of diagonal degrees matrix\n",
    "        dinv_mat = np.nan_to_num(np.diag(adj_mat.sum(axis=0)))\n",
    "\n",
    "        # compute final matrix, for details see (1) in Graph-BERT by Zhang et al. '20 page 3\n",
    "        Abar = np.matmul(adj_mat, dinv_mat)\n",
    "        I = np.diag(np.ones(n))\n",
    "        M = np.linalg.inv(I - (1-alpha)*Abar)\n",
    "\n",
    "        self.intimacy_mat = alpha*M\n",
    "    \n",
    "    def build_contexts(self):\n",
    "        context_list = np.zeros((self.n, self.context_size+1), dtype=np.int)\n",
    "\n",
    "        # always include target node into its context\n",
    "        context_list[:,0] = range(self.n)\n",
    "\n",
    "        # context of a node contains the topk nodes sorted by intimacy score\n",
    "        context_list[:,1:] = (-self.intimacy_mat).argsort(axis=1)[:,:self.context_size]\n",
    "        \n",
    "        self.context_list = context_list\n",
    "    \n",
    "    def build_wl_coloring(self):\n",
    "        G = nx.from_edgelist(self.edge_list, create_using=nx.Graph)\n",
    "\n",
    "        # initialize node colors\n",
    "        wl_colors = {node:1 for node in G.nodes}\n",
    "\n",
    "        for _ in range(self.wl_iterations):\n",
    "            for node in sorted(G.nodes):\n",
    "                # combine colors from neighbors\n",
    "                code_list = [wl_colors[node]] + [wl_colors[x] for x in sorted(G.neighbors(node))]\n",
    "                code = \"\".join(map(str, code_list))\n",
    "\n",
    "                # update node code\n",
    "                wl_colors[node] = hashlib.md5(code.encode()).hexdigest()\n",
    "        \n",
    "        color_to_num = {color:i for i,color in enumerate(set(wl_colors.values()))}\n",
    "        wl_colors = {node:color_to_num[c] for node,c in wl_colors.items()}\n",
    "        self.wl_colors = wl_colors\n",
    "    \n",
    "    def position_embed(self, v, col):\n",
    "        x = np.zeros(self.hidden_size)\n",
    "        dim = len(v)\n",
    "\n",
    "        for i in range(dim//2):\n",
    "            x[i] = np.math.sin(col/(10000**((2*i)/dim)))\n",
    "            x[i+1] = np.math.cos(col/10000**((2*i+1)/dim))\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\" each node together with its context represent an instance of the graph \"\"\"\n",
    "        return self.n\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # ids of nodes in the context\n",
    "        context = self.context_list[idx,:]\n",
    "\n",
    "        n = len(context)\n",
    "        \n",
    "        # raw feature vector embedding\n",
    "        X = torch.tensor([self.raw_features.loc[self.id_to_name[x]].to_numpy() for x in context])\n",
    "\n",
    "        # Weisfeiler-Lehman absolute role embedding\n",
    "        C = torch.tensor([self.wl_colors[self.id_to_name[i]] for i in context])\n",
    "\n",
    "        # intimacy based relative positional embedding\n",
    "        I = torch.tensor(range(n))\n",
    "\n",
    "        # hop based relative distance embedding\n",
    "        source = context[0]\n",
    "        H = torch.tensor([self.distance_matrix[source][dest] for dest in context])\n",
    "\n",
    "        # labels\n",
    "        y = torch.tensor([self.labels[self.id_to_name[x]] for x in context])\n",
    "        \n",
    "        return X, C, I, H, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marco\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3357: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    }
   ],
   "source": [
    "cora = CoraDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 1, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]),\n",
       " tensor([ 562,  983,  176, 1295,  562, 1630,  710,  818,  531,  508, 1207]),\n",
       " tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10]),\n",
       " tensor([0, 5, 3, 5, 0, 4, 6, 4, 9, 4, 5]),\n",
       " tensor([0, 1, 3, 1, 0, 4, 3, 5, 3, 1, 5], dtype=torch.int8))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cora[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphBertConfig():\n",
    "    def __init__(\n",
    "        self,\n",
    "        residual_type = 'none',\n",
    "        x_size=3000,\n",
    "        y_size=7,\n",
    "        k=5,\n",
    "        max_wl_role_index = 100,\n",
    "        max_hop_dis_index = 100,\n",
    "        max_inti_pos_index = 100,\n",
    "        hidden_size=32,\n",
    "        num_hidden_layers=1,\n",
    "        num_attention_heads=1,\n",
    "        intermediate_size=32,\n",
    "        hidden_act=\"gelu\",\n",
    "        hidden_dropout_prob=0.5,\n",
    "        attention_probs_dropout_prob=0.3,\n",
    "        initializer_range=0.02,\n",
    "        layer_norm_eps=1e-12,\n",
    "        is_decoder=False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(GraphBertConfig, self).__init__(**kwargs)\n",
    "        self.max_wl_role_index = max_wl_role_index\n",
    "        self.max_hop_dis_index = max_hop_dis_index\n",
    "        self.max_inti_pos_index = max_inti_pos_index\n",
    "        self.residual_type = residual_type\n",
    "        self.x_size = x_size\n",
    "        self.y_size = y_size\n",
    "        self.k = k\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.hidden_act = hidden_act\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.hidden_dropout_prob = hidden_dropout_prob\n",
    "        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
    "        self.initializer_range = initializer_range\n",
    "        self.layer_norm_eps = layer_norm_eps\n",
    "        self.is_decoder = is_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GraphBertConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEmbeddings(nn.Module):\n",
    "    def __init__(self, config: GraphBertConfig):\n",
    "        super().__init__()\n",
    "        self.raw_features_embeddings = nn.Linear(config.x_size, config.hidden_size)\n",
    "        self.wl_absolute_role_embeddings = nn.Embedding(config.max_wl_role_index, config.hidden_size)\n",
    "        self.intimacy_relative_embeddings = nn.Embedding(config.max_inti_pos_index, config.hidden_size)\n",
    "        self.hop_distance_embeddings = nn.Embedding(config.max_hop_dis_index, config.hidden_size)\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "    \n",
    "    def forward(self, raw_features, wl_role_ids, init_pos_ids, hop_dis_ids):\n",
    "        raw_feature_embeds = self.raw_feature_embeddings(raw_features)\n",
    "        role_embeddings = self.wl_role_embeddings(wl_role_ids)\n",
    "        position_embeddings = self.inti_pos_embeddings(init_pos_ids)\n",
    "        hop_embeddings = self.hop_dis_embeddings(hop_dis_ids)\n",
    "\n",
    "        embeddings = raw_feature_embeds + role_embeddings + position_embeddings + hop_embeddings\n",
    "        embeddings = self.layer_norm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'code.DatasetLoader'; 'code' is not a package",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-377cb879b59f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data.pickle\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'code.DatasetLoader'; 'code' is not a package"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"data.pickle\", \"rb\") as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "00ff9db9870ec9f1423bc268ca660a602054e7415f81d5798dee84cbde947ffd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
